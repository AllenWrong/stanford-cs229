{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://cs229.stanford.edu/ps/ps1/ps1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poisson distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "p(y; \\lambda)\n",
    "&= \\frac{e^{-\\lambda} \\lambda ^y}{y!} \\\\\n",
    "&= \\frac{1}{y!}\\exp \\big(y \\ln{\\lambda} - \\lambda\\big) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the form of exponential family $p(y;\\eta) = b(y) \\exp\\big(\\eta T(y) - a(\\eta)\\big) $, it becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "b(y) &= \\frac{1}{y!} \\\\\n",
    "\\eta &= \\ln \\lambda \\\\\n",
    "T(y) &= y \\\\\n",
    "a(\\eta) &= \\lambda = e^{\\eta} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canoical response function, i.e. the function ($g$) that gives the distribution’s mean as a function of the natural parameter ($\\eta$) based on https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "g(\\eta)\n",
    "&= \\mathbb{E}[T(y); \\eta] \\\\\n",
    "&= \\mathbb{E}[y; \\eta] \\\\\n",
    "&= \\lambda \\\\\n",
    "&= e^{\\eta} \\\\\n",
    "&= e^{\\theta^T x} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note $\\eta = \\theta^Tx$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, it can be confusing that the names for *cannoical response function* and *canonical link function* may be exchanged in what's found in other text books, e.g. [Machine Learning: A Probabilistic Perspective (Adaptive Computation and Machine Learning series)](https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation-ebook/dp/B00AF1AYTQ) Chapter 9. But according to the cs229 note, the convention adopted here is justified by\n",
    "\n",
    ">Many texts use $g$ to denote the link function, and $g^{−1}$ to denote the response function; but the notation we’re using here, inherited from the early machine learning literature, will be more consistent with the notation used in the rest of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-likelihood:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "l(\\theta) &= \\log p(y; \\lambda) \\\\\n",
    "          &= \\log \\frac{1}{y!} + y \\ln \\lambda - \\lambda \\\\\n",
    "          &= \\log \\frac{1}{y!} + y \\eta - e^{\\eta} \\\\\n",
    "          &= \\log \\frac{1}{y!} + y \\theta^T x - e^{\\theta^T x}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its derivative:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial l(\\theta)}{\\partial \\theta_j}\n",
    "&= y x_j - e^{\\theta^T x} x_j \\\\\n",
    "&= \\big(y - e^{\\theta^T x}\\big) x_j \\\\\n",
    "&= \\big(y - e^{\\eta}\\big) x_j \\\\\n",
    "&= (y - \\lambda) x_j\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the stochastic gradient ascent rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\theta_j &:= \\theta_j + \\alpha \\big(y^{(i)} - e^{\\theta^T x^{(i)}}\\big) x_j^{(i)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalized Linear Model form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y;\\eta) = b(y) \\exp\\big(\\eta T(y) - a(\\eta)\\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $T(y) = y$, so the GLM becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y;\\eta) = b(y) \\exp\\big(\\eta y - a(\\eta)\\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the log-likelihood is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\log p(y;\\eta) = \\log b(y) + \\eta y - a(\\eta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial \\log p(y;\\eta)}{\\partial \\theta_j} \n",
    "&= 0 + y \\frac{\\partial \\eta}{\\theta_j} - \\frac{\\partial a(\\eta)}{\\partial \\eta} \\frac{\\partial \\eta}{\\theta_j} \\\\\n",
    "&= y x_j  - \\frac{\\partial a(\\eta)}{\\partial \\eta} x_j \\\\\n",
    "&= \\left(y - \\frac{\\partial a(\\eta)}{\\partial \\eta}\\right) x_j\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to work out what $\\frac{\\partial a(\\eta)}{\\partial \\eta}$ is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*} \n",
    "\\frac{\\partial a(\\eta)}{\\partial \\eta}\n",
    "&= \\frac{\\partial \\log Z(\\eta)}{\\partial \\eta} \\\\\n",
    "&= \\frac{1}{Z} \\frac{\\partial Z}{\\partial \\eta} \\\\\n",
    "&= \\frac{1}{Z} \\frac{\\partial \\int b(y) \\exp(\\eta y) dy }{\\partial \\eta} \\\\\n",
    "&= \\frac{1}{Z} \\int y b(y) \\exp(\\eta y) dy \\\\\n",
    "&= \\mathbb{E}[y; \\eta] = g(\\eta) = e^{\\theta^T x} = h(x)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note,\n",
    "* the first equality used the fact that $a(\\eta) = \\log Z(\\eta)$ for the exponetial family, the former is known as the log partition function, while the later is the partition function.\n",
    "* the last equality used the result from section (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial a(\\eta)}{\\partial \\eta} = h(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking it back to the derivative,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial \\log p(y;\\eta)}{\\partial \\theta_j}  = \\big(y - h(x)\\big) x_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the stochastic gradient ascent rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\theta_j \n",
    "&:= \\theta_j + \\alpha \\big(y^{(i)} - h(x^{(i)} \\big)x_j^{(i)} \\\\\n",
    "&:= \\theta_j - \\alpha \\big(h(x^{(i)} - y^{(i)} \\big)x_j^{(i)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very interesting to know that all GLM where $T(y) = y$ have the same update rule for stochastic gradient ascent."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
